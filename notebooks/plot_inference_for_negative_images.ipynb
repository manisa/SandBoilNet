{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a24853",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 18:59:37.014138: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-18 18:59:37.639498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mpanta1/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "Keras version: 2.12.0\n",
      "Eager mode enabled: True\n",
      "Num GPUs Available:  1\n",
      "Num CPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "import sys; sys.path.insert(0, '..') # add parent folder path where lib folder is\n",
    "from lib.metrics import create_dir\n",
    "from lib.load_data import get_data\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "\n",
    "print('TensorFlow version: {version}'.format(version=tf.__version__))\n",
    "print('Keras version: {version}'.format(version=tf.keras.__version__))\n",
    "print('Eager mode enabled: {mode}'.format(mode=tf.executing_eagerly()))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb70eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCALayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_components, **kwargs):\n",
    "        super(PCALayer, self).__init__(**kwargs)\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.shape = input_shape\n",
    "        self.input_dim = int(input_shape[-1])\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(self.input_dim, self.n_components), dtype=\"float32\",\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Flatten the input tensor\n",
    "        #x = tf.linalg.normalize(x,axis=-1)\n",
    "        #print(x.shape)\n",
    "        # assumption is that the feature vector is normalized\n",
    "        #x = tf.math.l2_normalize(x, axis=-1)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        flattened = tf.reshape(x, [batch_size, -1, self.input_dim])\n",
    "        \n",
    "        # Compute the mean and subtract it from the input tensor\n",
    "        mean = tf.reduce_mean(flattened, axis=1, keepdims=True)\n",
    "        centered = flattened - mean\n",
    "        \n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        cov = tf.matmul(centered, centered, transpose_a=True) / tf.cast(tf.shape(flattened)[1] - 1, tf.float32)\n",
    "\n",
    "        # Compute the eigenvalues and eigenvectors of the covariance matrix\n",
    "        eigenvalues, eigenvectors = tf.linalg.eigh(cov)\n",
    "\n",
    "        # Sort the eigenvectors based on the eigenvalues\n",
    "        idx = tf.argsort(eigenvalues, axis=-1, direction='DESCENDING')\n",
    "        top_eigenvectors = tf.gather(eigenvectors, idx, batch_dims=1, axis=-1)\n",
    "        top_eigenvectors = top_eigenvectors[:, :, :self.n_components]\n",
    "\n",
    "        # Transpose the eigenvectors to match the input shape\n",
    "        top_eigenvectors = tf.transpose(top_eigenvectors, perm=[0, 1, 2])\n",
    "        \n",
    "        # Project centered data onto top principal components\n",
    "        projected = tf.matmul(centered, top_eigenvectors)\n",
    "\n",
    "        # Reshape projected data and return as output\n",
    "        output_shape = tf.concat([tf.shape(x)[:-1], [self.n_components]], axis=0)\n",
    "        #output = tf.reshape(projected, shape=(-1, *self.output_shape))\n",
    "        output = tf.reshape(projected, output_shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple(input_shape[:-1]) + (self.n_components,)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PCALayer, self).get_config()\n",
    "        config.update({'n_components': self.n_components})\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84803102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prediction(test_ids, image_path, mask_path, model_dict):\n",
    "    # empty list to store results\n",
    "    image_names, image_id, mask_id, has_mask = [], [], [], []\n",
    "\n",
    "    results_dict = {name: [] for name in model_dict.keys()}\n",
    "    \n",
    "    image_path = os.path.abspath(image_path)\n",
    "    mask_path = os.path.abspath(mask_path)\n",
    "\n",
    "    # Iterating through each image in test data\n",
    "    for n, i in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "        filename = i\n",
    "        image_names.append(filename)\n",
    "        i = os.path.join(image_path, i)\n",
    "\n",
    "        # Creating an empty array of shape 1,512,512,3\n",
    "        X = np.empty((1,512,512,3))\n",
    "        # Read the image\n",
    "        img = io.imread(i)\n",
    "        # Resizing the image and converting them to array of type float64\n",
    "        img = cv2.resize(img, (512,512))\n",
    "        img = np.array(img, dtype=np.float64)\n",
    "        \n",
    "        # Standardising the image\n",
    "        img /= 255.0\n",
    "        # Converting the shape of image from 512,512,3 to 1,512,512,3\n",
    "        X[0,] = img\n",
    "        \n",
    "        # Make prediction of mask\n",
    "        for model_name, model in model_dict.items():\n",
    "            start_time = time.time()\n",
    "            prediction = model.predict(X, verbose=0)\n",
    "            prediction = K.greater_equal(prediction, 0.5)\n",
    "            prediction = np.round(prediction,0)\n",
    "            end_time = time.time()\n",
    "            eval_time = end_time - start_time\n",
    "            #print(f'eval time for {model_name} is {eval_time}seconds')\n",
    "            results_dict[model_name].append(prediction)\n",
    "        \n",
    "        image_id.append(os.path.join(image_path, filename))\n",
    "        mask_id.append(os.path.join(mask_path, filename.split('.')[0] + '.png'))\n",
    "        has_mask.append(1)\n",
    "            \n",
    "    return pd.DataFrame({'file_name': image_names, 'image_path': image_id, \n",
    "                         'mask_path': mask_id, 'has_mask': has_mask, **results_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a533a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(y_true_in, y_pred_in):\n",
    "    smooth = 1e-15\n",
    "    intersection = y_true_in.ravel() * y_pred_in.ravel()\n",
    "    union = y_true_in.ravel() + y_pred_in.ravel() - intersection\n",
    "\n",
    "    iou = ((np.sum(intersection) + smooth)/(np.sum(union) + smooth))\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd38b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_visualization(df_pred, filename, model_dict, num_images):\n",
    "    num_models = len(model_dict)\n",
    "    fig, axs = plt.subplots(num_images, num_models + 1, figsize=(6 * (num_models + 1), 6 * num_images))\n",
    "    W, H = 512, 512\n",
    "    dim = (W, H)\n",
    "\n",
    "    root_dir = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "    filepath = os.path.join(root_dir, \"results\", filename)\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.9, wspace=0.1, hspace=0.3)\n",
    "\n",
    "    for count, row in tqdm(enumerate(df_pred.itertuples())):\n",
    "        if row.has_mask == 1 and count < num_images:\n",
    "            img = cv2.imread(row.image_path, cv2.IMREAD_COLOR)\n",
    "            img = cv2.resize(img, dim)\n",
    "            axs[count, 0].imshow(img)\n",
    "            axs[count, 0].axis('off')\n",
    "            axs[count, 0].set_title('Original Image ' + str(count + 1), fontsize=48, pad=10)\n",
    "\n",
    "            \"\"\"   mask = cv2.imread(row.mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, dim)\n",
    "            img[mask == 255] = (200, 0, 0)\n",
    "            axs[count, 1].imshow(img)\n",
    "            axs[count, 1].axis('off')\n",
    "            axs[count, 1].set_title('Ground Truth ' + str(count + 1), fontsize=48, pad=10)\n",
    "            mask = np.array(mask, dtype=np.float64)\n",
    "            mask = mask / 255 \"\"\"\n",
    "\n",
    "            for j, (model_name, model) in enumerate(model_dict.items()):\n",
    "                pred = np.array(row._asdict()[model_name])\n",
    "                #iou = iou_metric(mask, pred)\n",
    "                pred = np.squeeze(pred)\n",
    "                pred = np.uint8(pred > 0.6) *255\n",
    "                img_model = cv2.imread(row.image_path, cv2.IMREAD_COLOR)\n",
    "                img_model = cv2.resize(img_model, (512, 512))\n",
    "                img_model = img_model.squeeze()\n",
    "                img_model[pred == 255] = [(204, 204, 0), (102, 204, 0), (104, 0, 104), (0, 204, 204), (0, 153, 153)][j]\n",
    "                axs[count, j + 1].imshow(img_model)\n",
    "                axs[count, j + 1].axis('off')\n",
    "                axs[count, j + 1].set_title(f'{model_name}', fontsize=42, pad=10)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(filepath, format='png', facecolor=\"w\", transparent=False)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35d09ac7",
   "metadata": {},
   "source": [
    "## Load data for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77749b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mpanta1/usace_workspace/sandboil_research\n",
      "/home/mpanta1/usace_workspace/sandboil_research/datasets/test_images/levee_crack_test\n",
      "['LCST16Jan18-006_20160124103700.jpg', 'pic_(16).jpg', 'pic_(4).jpg', 'LKBO15Jun26-001_20150803101000.jpg', 'pic_(8).jpg', '0171.png', 'LWMS11May11-006_20110522155200.jpg', '0021.png', '0102.png']\n"
     ]
    }
   ],
   "source": [
    "img_height = 512\n",
    "img_width = 512\n",
    "import random\n",
    "root_dir = os.path.normpath(os.getcwd() + os.sep + os.pardir + os.sep + os.pardir)\n",
    "print(root_dir)\n",
    "#path_test_levee = os.path.join(root_dir,  \"datasets\", 'original_dataset' ,\"test\")\n",
    "path_test_levee = os.path.join(root_dir,  \"datasets\", \"test_images\", \"levee_crack_test\") #exp2_test, pothole_test, with_grassland, #no_grassland inside \"test_images\"\n",
    "\n",
    "print(path_test_levee)\n",
    "\n",
    "test_images_leveecrack = sorted(next(os.walk(path_test_levee + \"/images\"))[2])\n",
    "\n",
    "random.Random(42).shuffle(test_images_leveecrack)\n",
    "\n",
    "\n",
    "\n",
    "print(test_images_leveecrack)\n",
    "image_leveecrack_path = path_test_levee + \"/images\"\n",
    "mask_leveecrack_path = path_test_levee + \"/masks\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c66beb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e57422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, custom_layer=False):\n",
    "    result_folder_name = str(model_name)\n",
    "    root_dir = os.path.normpath(os.getcwd() + os.sep + os.pardir)\n",
    "    model_path = os.path.join(root_dir, \"models/state_of_the_art_comparison_bce_dice\", str(model_name), \"best_model.h5\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    if custom_layer:\n",
    "        best_model = tf.keras.models.load_model(model_path, custom_objects={'PCALayer': PCALayer}, compile=False)\n",
    "        \n",
    "    else:\n",
    "        print(\"here\")\n",
    "        best_model = tf.keras.models.load_model(model_path,compile=False)\n",
    "    print(f'=========Loaded {model_name}===========')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "764e0073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "=========Loaded unet_bce_dice_loss_new===========\n",
      "here\n",
      "=========Loaded multiresunet_bce_dice_loss_new===========\n",
      "here\n",
      "=========Loaded attentionunet_bce_dice_loss_new===========\n",
      "here\n",
      "=========Loaded nestedunet_bce_dice_loss_new===========\n",
      "=========Loaded SandboilNet_negative_23_bce_dice_loss_new===========\n"
     ]
    }
   ],
   "source": [
    "M0 = load_model('unet_bce_dice_loss_new', False)\n",
    "M1 = load_model('multiresunet_bce_dice_loss_new', False)\n",
    "M2 = load_model('attentionunet_bce_dice_loss_new', False)\n",
    "M3 = load_model('nestedunet_bce_dice_loss_new', False)\n",
    "M4 = load_model('SandboilNet_negative_23_bce_dice_loss_new', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b146f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 184 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd2141f4e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 185 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd2142e1280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:11<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow.keras.backend as K\n",
    "model_dict = {'M1':M0, 'M2':M1, 'M3':M2, 'M4':M3,'M5':M4 }\n",
    "#model_dict = {'M1' : M4}\n",
    "\n",
    "df_all_preds_all = []\n",
    "df_all_preds_all = prediction(test_images_leveecrack, os.path.abspath(image_leveecrack_path), os.path.abspath(mask_leveecrack_path), model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaed3919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:01,  5.06it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_images = 9\n",
    "show_visualization(df_all_preds_all, \"all_negative_images.png\", model_dict, num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_visualization = df_all_preds_all.sample(frac = 0.98)\n",
    "show_visualization(df_for_visualization, \"four_models_inferences_98.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4fd27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
